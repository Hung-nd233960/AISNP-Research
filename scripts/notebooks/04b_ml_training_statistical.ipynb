{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set project root\n",
    "project_root = Path(\"/home/Plutonium/Documents/BioinfoMidterm\")\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Project imports\n",
    "from config import PATHS, ML\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd0e9b",
   "metadata": {},
   "source": [
    "## Step 1: Load Statistical Results and ML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statistical SNP scores from 03b\n",
    "stats_path = str(PATHS.OUTPUT_DIR / \"statistics_snp_scores.csv\")\n",
    "print(f\"Loading statistical results from: {stats_path}\")\n",
    "\n",
    "stats_df = pd.read_csv(stats_path)\n",
    "print(f\"Statistical results shape: {stats_df.shape}\")\n",
    "print(f\"\\nColumns: {list(stats_df.columns)}\")\n",
    "\n",
    "# Load ML data (genotype matrix with population labels)\n",
    "ml_data_path = str(PATHS.ML_DATA)\n",
    "print(f\"\\nLoading ML data from: {ml_data_path}\")\n",
    "\n",
    "df = pd.read_csv(ml_data_path)\n",
    "print(f\"ML data shape: {df.shape}\")\n",
    "print(f\"\\nPopulation distribution:\")\n",
    "print(df['pop'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "sample_ids = df['sample']\n",
    "populations = df['pop']\n",
    "snp_columns = [c for c in df.columns if c not in ['sample', 'pop']]\n",
    "X_all = df[snp_columns]\n",
    "y = populations\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total SNPs in ML data: {len(snp_columns)}\")\n",
    "print(f\"\\nFirst 5 SNP columns: {snp_columns[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafbc27",
   "metadata": {},
   "source": [
    "## Step 2: Identify Consensus SNPs (Significant in All 4 Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define significance criteria for each test\n",
    "# Using FDR-corrected significance for chi2, and top 500 for information-theoretic measures\n",
    "\n",
    "# Check if significance columns exist\n",
    "if 'chi2_significant_fdr' in stats_df.columns:\n",
    "    sig_chi2 = set(stats_df[stats_df['chi2_significant_fdr']]['snp_id'])\n",
    "else:\n",
    "    # Fallback: use top 500 by chi2\n",
    "    sig_chi2 = set(stats_df.nlargest(500, 'chi2')['snp_id'])\n",
    "\n",
    "sig_mi = set(stats_df.nlargest(500, 'mutual_information')['snp_id'])\n",
    "sig_ig = set(stats_df.nlargest(500, 'information_gain')['snp_id'])\n",
    "sig_kl = set(stats_df.nlargest(500, 'kl_divergence')['snp_id'])\n",
    "\n",
    "print(\"SNPs Selected by Each Test:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"χ² test (FDR significant or top 500): {len(sig_chi2)}\")\n",
    "print(f\"Mutual Information (top 500): {len(sig_mi)}\")\n",
    "print(f\"Information Gain (top 500): {len(sig_ig)}\")\n",
    "print(f\"KL Divergence (top 500): {len(sig_kl)}\")\n",
    "\n",
    "# Count SNPs appearing in multiple tests\n",
    "all_sig_snps = list(sig_chi2) + list(sig_mi) + list(sig_ig) + list(sig_kl)\n",
    "snp_counts = Counter(all_sig_snps)\n",
    "\n",
    "# Get consensus SNPs\n",
    "snps_in_4 = [s for s, c in snp_counts.items() if c == 4]\n",
    "snps_in_3 = [s for s, c in snp_counts.items() if c >= 3]\n",
    "snps_in_2 = [s for s, c in snp_counts.items() if c >= 2]\n",
    "\n",
    "print(f\"\\nConsensus SNPs:\")\n",
    "print(f\"  Significant in ALL 4 tests: {len(snps_in_4)}\")\n",
    "print(f\"  Significant in ≥3 tests: {len(snps_in_3)}\")\n",
    "print(f\"  Significant in ≥2 tests: {len(snps_in_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select consensus SNPs for training\n",
    "# Use SNPs significant in all 4 tests if enough, otherwise use ≥3\n",
    "\n",
    "if len(snps_in_4) >= 50:\n",
    "    consensus_snps = snps_in_4\n",
    "    consensus_level = \"all 4 tests\"\n",
    "elif len(snps_in_3) >= 50:\n",
    "    consensus_snps = snps_in_3\n",
    "    consensus_level = \"≥3 tests\"\n",
    "else:\n",
    "    consensus_snps = snps_in_2\n",
    "    consensus_level = \"≥2 tests\"\n",
    "\n",
    "print(f\"Using {len(consensus_snps)} SNPs significant in {consensus_level}\")\n",
    "\n",
    "# Filter to SNPs that exist in ML data\n",
    "available_consensus = [s for s in consensus_snps if s in snp_columns]\n",
    "print(f\"Available in ML data: {len(available_consensus)} SNPs\")\n",
    "\n",
    "# If still not enough, use top SNPs by composite score\n",
    "if len(available_consensus) < 25:\n",
    "    print(\"\\nNot enough consensus SNPs. Using top SNPs by composite score...\")\n",
    "    # Create composite score (rank average)\n",
    "    stats_df['rank_chi2'] = stats_df['chi2'].rank(ascending=False)\n",
    "    stats_df['rank_mi'] = stats_df['mutual_information'].rank(ascending=False)\n",
    "    stats_df['rank_ig'] = stats_df['information_gain'].rank(ascending=False)\n",
    "    stats_df['rank_kl'] = stats_df['kl_divergence'].rank(ascending=False)\n",
    "    stats_df['composite_rank'] = (stats_df['rank_chi2'] + stats_df['rank_mi'] + \n",
    "                                   stats_df['rank_ig'] + stats_df['rank_kl']) / 4\n",
    "    \n",
    "    top_composite = stats_df.nsmallest(500, 'composite_rank')['snp_id'].tolist()\n",
    "    available_consensus = [s for s in top_composite if s in snp_columns]\n",
    "    print(f\"Using top {len(available_consensus)} SNPs by composite rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data with consensus SNPs\n",
    "X_consensus = X_all[available_consensus]\n",
    "\n",
    "print(f\"\\nConsensus Training Data:\")\n",
    "print(f\"  Samples: {X_consensus.shape[0]}\")\n",
    "print(f\"  Features (consensus SNPs): {X_consensus.shape[1]}\")\n",
    "print(f\"\\nFirst 10 consensus SNPs:\")\n",
    "for i, snp in enumerate(available_consensus[:10], 1):\n",
    "    print(f\"  {i}. {snp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe68bd",
   "metadata": {},
   "source": [
    "## Step 3: Train Random Forest on Consensus SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_consensus, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Encode labels for XGBoost compatibility\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, range(len(le.classes_))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b73eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest on consensus SNPs\n",
    "print(\"Training Random Forest on consensus SNPs...\")\n",
    "\n",
    "rf_consensus = RandomForestClassifier(\n",
    "    n_estimators=ML.RF_N_ESTIMATORS,\n",
    "    max_depth=ML.RF_MAX_DEPTH,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_consensus.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_consensus = rf_consensus.predict(X_test)\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred_consensus)\n",
    "\n",
    "print(f\"\\nRandom Forest on Consensus SNPs ({len(available_consensus)} features):\")\n",
    "print(f\"  Accuracy: {accuracy_consensus:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_consensus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ee6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances and select top 25\n",
    "feature_importance = pd.DataFrame({\n",
    "    'snp': available_consensus,\n",
    "    'importance': rf_consensus.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Add statistical scores\n",
    "feature_importance = feature_importance.merge(\n",
    "    stats_df[['snp_id', 'chi2', 'mutual_information', 'information_gain', 'kl_divergence']],\n",
    "    left_on='snp', right_on='snp_id', how='left'\n",
    ").drop('snp_id', axis=1)\n",
    "\n",
    "print(\"Top 25 SNPs by Random Forest Importance:\")\n",
    "print(\"=\"*80)\n",
    "display(feature_importance.head(25))\n",
    "\n",
    "# Get top 25 SNP IDs\n",
    "TOP_N = ML.TOP_N_FEATURES  # 25\n",
    "top_25_snps = feature_importance.head(TOP_N)['snp'].tolist()\n",
    "\n",
    "print(f\"\\nSelected {TOP_N} top SNPs for final model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot 1: Top 25 RF importance\n",
    "ax = axes[0]\n",
    "top_25_df = feature_importance.head(25)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, 25))\n",
    "bars = ax.barh(range(25), top_25_df['importance'].values, color=colors)\n",
    "ax.set_yticks(range(25))\n",
    "ax.set_yticklabels(top_25_df['snp'].values, fontsize=8)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Top 25 SNPs: Random Forest Importance\\n(Trained on Consensus SNPs)')\n",
    "\n",
    "# Plot 2: Importance vs MI scatter\n",
    "ax = axes[1]\n",
    "ax.scatter(feature_importance['mutual_information'], \n",
    "           feature_importance['importance'], alpha=0.5, s=20)\n",
    "# Highlight top 25\n",
    "top_25_mask = feature_importance['snp'].isin(top_25_snps)\n",
    "ax.scatter(feature_importance.loc[top_25_mask, 'mutual_information'],\n",
    "           feature_importance.loc[top_25_mask, 'importance'],\n",
    "           color='red', s=50, label='Top 25', edgecolors='black')\n",
    "ax.set_xlabel('Mutual Information (Statistical)')\n",
    "ax.set_ylabel('RF Feature Importance (ML)')\n",
    "ax.set_title('Statistical vs ML-based SNP Ranking')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ddabe7",
   "metadata": {},
   "source": [
    "## Step 4: Train Multiple Classifiers on Top 25 SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88070a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data with top 25 SNPs\n",
    "X_top25 = X_all[top_25_snps]\n",
    "\n",
    "# Train/test split\n",
    "X_train_25, X_test_25, y_train_25, y_test_25 = train_test_split(\n",
    "    X_top25, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "y_train_25_enc = le.fit_transform(y_train_25)\n",
    "y_test_25_enc = le.transform(y_test_25)\n",
    "\n",
    "# Scale for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_25_scaled = scaler.fit_transform(X_train_25)\n",
    "X_test_25_scaled = scaler.transform(X_test_25)\n",
    "\n",
    "print(f\"Top 25 Training Data:\")\n",
    "print(f\"  Training: {X_train_25.shape}\")\n",
    "print(f\"  Test: {X_test_25.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bda45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=ML.RF_N_ESTIMATORS, max_depth=ML.RF_MAX_DEPTH, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=ML.XGB_N_ESTIMATORS, max_depth=ML.XGB_MAX_DEPTH,\n",
    "        learning_rate=ML.XGB_LEARNING_RATE, random_state=42, n_jobs=-1, verbosity=0\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, random_state=42, multi_class='multinomial', n_jobs=-1\n",
    "    ),\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf', random_state=42, probability=True\n",
    "    ),\n",
    "    'SVM (Linear)': SVC(\n",
    "        kernel='linear', random_state=42, probability=True\n",
    "    ),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=5, n_jobs=-1\n",
    "    ),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=5, random_state=42\n",
    "    ),\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=100, random_state=42, algorithm='SAMME'\n",
    "    ),\n",
    "    'MLP Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, early_stopping=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Training {len(classifiers)} classifiers on top {TOP_N} statistical SNPs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all classifiers\n",
    "results = []\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Select appropriate data (scaled vs unscaled, encoded vs string labels)\n",
    "    if 'SVM' in name or 'Logistic' in name or 'KNN' in name or 'MLP' in name:\n",
    "        X_tr, X_te = X_train_25_scaled, X_test_25_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train_25.values, X_test_25.values\n",
    "    \n",
    "    # Use encoded labels for XGBoost and MLP\n",
    "    if 'XGBoost' in name or 'MLP' in name:\n",
    "        y_tr, y_te = y_train_25_enc, y_test_25_enc\n",
    "    else:\n",
    "        y_tr, y_te = y_train_25, y_test_25\n",
    "    \n",
    "    try:\n",
    "        # Train\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_te)\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_te, y_pred)\n",
    "        \n",
    "        # Convert back for consistent reporting\n",
    "        if 'XGBoost' in name or 'MLP' in name:\n",
    "            y_te_str = le.inverse_transform(y_te)\n",
    "            y_pred_str = le.inverse_transform(y_pred)\n",
    "        else:\n",
    "            y_te_str = y_te\n",
    "            y_pred_str = y_pred\n",
    "        \n",
    "        prec = precision_score(y_te_str, y_pred_str, average='weighted')\n",
    "        rec = recall_score(y_te_str, y_pred_str, average='weighted')\n",
    "        f1 = f1_score(y_te_str, y_pred_str, average='weighted')\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(clf, X_tr, y_tr, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1-Score': f1,\n",
    "            'CV Mean': cv_scores.mean(),\n",
    "            'CV Std': cv_scores.std()\n",
    "        })\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f} | CV: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': np.nan,\n",
    "            'Precision': np.nan,\n",
    "            'Recall': np.nan,\n",
    "            'F1-Score': np.nan,\n",
    "            'CV Mean': np.nan,\n",
    "            'CV Std': np.nan\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451a7f9",
   "metadata": {},
   "source": [
    "## Step 5: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison charts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy ranking\n",
    "ax = axes[0, 0]\n",
    "sorted_results = results_df.sort_values('Accuracy', ascending=True)\n",
    "colors = plt.cm.RdYlGn(sorted_results['Accuracy'].values / sorted_results['Accuracy'].max())\n",
    "bars = ax.barh(sorted_results['Model'], sorted_results['Accuracy'], color=colors)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Model Accuracy Ranking (Top 25 Statistical SNPs)')\n",
    "ax.set_xlim(0, 1)\n",
    "for i, (acc, model) in enumerate(zip(sorted_results['Accuracy'], sorted_results['Model'])):\n",
    "    ax.text(acc + 0.01, i, f'{acc:.3f}', va='center', fontsize=9)\n",
    "\n",
    "# Plot 2: F1-Score comparison\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, results_df['Precision'], width, label='Precision', color='steelblue')\n",
    "ax.bar(x + width/2, results_df['Recall'], width, label='Recall', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right', fontsize=8)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision vs Recall by Model')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3: CV Score with error bars\n",
    "ax = axes[1, 0]\n",
    "sorted_cv = results_df.sort_values('CV Mean', ascending=True)\n",
    "ax.barh(sorted_cv['Model'], sorted_cv['CV Mean'], xerr=sorted_cv['CV Std'],\n",
    "        color='teal', alpha=0.7, capsize=3)\n",
    "ax.set_xlabel('Cross-Validation Accuracy')\n",
    "ax.set_title('5-Fold Cross-Validation Scores')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Plot 4: Accuracy vs CV (generalization check)\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(results_df['Accuracy'], results_df['CV Mean'], s=100, c='teal', edgecolors='black')\n",
    "for i, row in results_df.iterrows():\n",
    "    ax.annotate(row['Model'], (row['Accuracy'], row['CV Mean']), \n",
    "                fontsize=8, ha='left', va='bottom')\n",
    "ax.plot([0.5, 1], [0.5, 1], 'r--', label='Perfect generalization')\n",
    "ax.set_xlabel('Test Accuracy')\n",
    "ax.set_ylabel('CV Accuracy')\n",
    "ax.set_title('Test vs Cross-Validation Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78042f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for top 3 models\n",
    "top_3_models = results_df.head(3)['Model'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, model_name in enumerate(top_3_models):\n",
    "    clf = classifiers[model_name]\n",
    "    \n",
    "    # Get appropriate data\n",
    "    if 'SVM' in model_name or 'Logistic' in model_name or 'KNN' in model_name or 'MLP' in model_name:\n",
    "        X_te = X_test_25_scaled\n",
    "    else:\n",
    "        X_te = X_test_25.values\n",
    "    \n",
    "    if 'XGBoost' in model_name or 'MLP' in model_name:\n",
    "        y_te = y_test_25_enc\n",
    "        y_pred = clf.predict(X_te)\n",
    "        y_te = le.inverse_transform(y_te)\n",
    "        y_pred = le.inverse_transform(y_pred)\n",
    "    else:\n",
    "        y_te = y_test_25\n",
    "        y_pred = clf.predict(X_te)\n",
    "    \n",
    "    cm = confusion_matrix(y_te, y_pred, labels=le.classes_)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    ax.set_title(f'{model_name}\\nAccuracy: {acc:.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215eacf",
   "metadata": {},
   "source": [
    "## Step 6: Compare with FST-based Selection (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load FST-based top SNPs for comparison\n",
    "try:\n",
    "    fst_top_snps_path = str(PATHS.TOP_SNPS_FILE)\n",
    "    fst_top_snps = pd.read_csv(fst_top_snps_path)['ID'].tolist()[:TOP_N]\n",
    "    \n",
    "    print(f\"Loaded {len(fst_top_snps)} FST-selected SNPs\")\n",
    "    \n",
    "    # Check overlap\n",
    "    overlap = set(top_25_snps) & set(fst_top_snps)\n",
    "    print(f\"\\nOverlap between methods:\")\n",
    "    print(f\"  Statistical top {TOP_N}: {len(top_25_snps)} SNPs\")\n",
    "    print(f\"  FST-based top {TOP_N}: {len(fst_top_snps)} SNPs\")\n",
    "    print(f\"  Shared SNPs: {len(overlap)} ({100*len(overlap)/TOP_N:.1f}%)\")\n",
    "    \n",
    "    if len(overlap) > 0:\n",
    "        print(f\"\\nShared SNPs:\")\n",
    "        for snp in list(overlap)[:10]:\n",
    "            print(f\"  - {snp}\")\n",
    "        if len(overlap) > 10:\n",
    "            print(f\"  ... and {len(overlap) - 10} more\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"FST comparison not available: {e}\")\n",
    "    print(\"Run 03_fst_and_pca.ipynb first to generate FST-selected SNPs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c102918",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model comparison results\n",
    "output_dir = PATHS.OUTPUT_DIR\n",
    "\n",
    "# Save results table\n",
    "results_path = str(output_dir / \"statistical_ml_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Model results saved to: {results_path}\")\n",
    "\n",
    "# Save top 25 SNPs with importance\n",
    "top25_path = str(output_dir / \"statistical_top25_snps.csv\")\n",
    "feature_importance.head(TOP_N).to_csv(top25_path, index=False)\n",
    "print(f\"Top 25 SNPs saved to: {top25_path}\")\n",
    "\n",
    "# Save top 25 SNP IDs for external use\n",
    "top25_ids_path = str(output_dir / \"statistical_top25_snp_ids.txt\")\n",
    "with open(top25_ids_path, 'w') as f:\n",
    "    for snp in top_25_snps:\n",
    "        f.write(f\"{snp}\\n\")\n",
    "print(f\"Top 25 SNP IDs saved to: {top25_ids_path}\")\n",
    "\n",
    "# Save report\n",
    "report_path = str(project_root / \"reports\" / \"statistical_ml_report.txt\")\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"ML TRAINING ON STATISTICALLY-SELECTED SNPs\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Dataset:\\n\")\n",
    "    f.write(f\"  Samples: {len(df)}\\n\")\n",
    "    f.write(f\"  Populations: {list(y.unique())}\\n\")\n",
    "    f.write(f\"  Consensus SNPs: {len(available_consensus)}\\n\")\n",
    "    f.write(f\"  Top SNPs used: {TOP_N}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Model Results (Top 25 SNPs):\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for _, row in results_df.iterrows():\n",
    "        f.write(f\"  {row['Model']:25} | Acc: {row['Accuracy']:.4f} | F1: {row['F1-Score']:.4f} | CV: {row['CV Mean']:.4f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nBest Model: {results_df.iloc[0]['Model']}\\n\")\n",
    "    f.write(f\"  Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\\n\")\n",
    "    f.write(f\"  F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\\n\")\n",
    "\n",
    "print(f\"Report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SNP ML TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Samples: {len(df)}\")\n",
    "print(f\"  Populations: {list(y.unique())}\")\n",
    "\n",
    "print(f\"\\nSNP Selection:\")\n",
    "print(f\"  Consensus SNPs (from 03b): {len(available_consensus)}\")\n",
    "print(f\"  Top SNPs for final training: {TOP_N}\")\n",
    "\n",
    "print(f\"\\nBest Performing Models:\")\n",
    "for i, row in results_df.head(3).iterrows():\n",
    "    print(f\"  {i+1}. {row['Model']}: {row['Accuracy']:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\nTop 5 SNPs by Importance:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {i+1}. {row['snp']} (importance: {row['importance']:.4f}, MI: {row['mutual_information']:.4f})\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - statistical_ml_results.csv\")\n",
    "print(f\"  - statistical_top25_snps.csv\")\n",
    "print(f\"  - statistical_top25_snp_ids.txt\")\n",
    "print(f\"  - reports/statistical_ml_report.txt\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  - Compare with FST-based results (04_ml_training.ipynb)\")\n",
    "print(f\"  - Use consensus SNPs from both methods for final model\")\n",
    "print(f\"  - Apply to external validation datasets\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
