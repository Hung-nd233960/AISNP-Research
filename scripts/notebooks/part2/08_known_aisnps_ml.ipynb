{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ebfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set project root\n",
    "project_root = Path(\"/home/Plutonium/Documents/BioinfoMidterm\")\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Part 2 imports\n",
    "from part2.ml_comparison import (\n",
    "    get_default_classifiers,\n",
    "    prepare_data,\n",
    "    run_model_comparison,\n",
    "    generate_confusion_matrices,\n",
    "    plot_confusion_matrices,\n",
    "    plot_performance_comparison,\n",
    "    create_summary_report\n",
    ")\n",
    "\n",
    "# Project imports\n",
    "from config import PATHS\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc65cfd",
   "metadata": {},
   "source": [
    "## Step 1: Load ML Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b741fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PART2_DIR = project_root / \"output\" / \"part2\"\n",
    "GRAPHS_DIR = project_root / \"graphs\" / \"part2\"\n",
    "GRAPHS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find all ML matrix files\n",
    "ml_files = list(PART2_DIR.glob(\"*_ml_matrix.csv\"))\n",
    "\n",
    "# Also include statistical SNPs from Part 1\n",
    "stat_path = PATHS.OUTPUT_DIR / \"statistical_ml_data_02b.csv\"\n",
    "\n",
    "print(f\"Found {len(ml_files)} ML matrices in Part 2:\")\n",
    "for f in ml_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "if stat_path.exists():\n",
    "    print(f\"\\nAlso including: statistical_ml_data_02b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8910296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all matrices\n",
    "datasets = {}\n",
    "\n",
    "for ml_file in ml_files:\n",
    "    name = ml_file.stem.replace('_ml_matrix', '')\n",
    "    df = pd.read_csv(ml_file)\n",
    "    datasets[name] = df\n",
    "    print(f\"Loaded {name}: {df.shape}\")\n",
    "\n",
    "# Add statistical SNPs\n",
    "if stat_path.exists():\n",
    "    stat_df = pd.read_csv(stat_path)\n",
    "    datasets['statistical_all4'] = stat_df\n",
    "    print(f\"Loaded statistical_all4: {stat_df.shape}\")\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798201ba",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview table\n",
    "overview = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    snp_cols = [c for c in df.columns if c not in ['sample', 'pop', 'source']]\n",
    "    overview.append({\n",
    "        'Source': name,\n",
    "        'Samples': len(df),\n",
    "        'SNPs': len(snp_cols),\n",
    "        'Populations': df['pop'].nunique(),\n",
    "        'Pop_List': ', '.join(df['pop'].unique())\n",
    "    })\n",
    "\n",
    "overview_df = pd.DataFrame(overview)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "display(overview_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: SNP counts\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(overview_df)))\n",
    "bars = ax.bar(overview_df['Source'], overview_df['SNPs'], color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Number of SNPs')\n",
    "ax.set_title('SNP Count by Source')\n",
    "ax.set_xticklabels(overview_df['Source'], rotation=45, ha='right')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, overview_df['SNPs']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(GRAPHS_DIR / 'snp_counts_by_source.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a908f",
   "metadata": {},
   "source": [
    "## Step 3: Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers for comparison\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "        random_state=42, n_jobs=-1, verbosity=0\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, random_state=42, multi_class='multinomial', n_jobs=-1\n",
    "    ),\n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf', random_state=42, probability=True\n",
    "    ),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=5, n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=5, random_state=42\n",
    "    ),\n",
    "    'MLP Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, early_stopping=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Classifiers: {len(classifiers)}\")\n",
    "for name in classifiers.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6bd4d",
   "metadata": {},
   "source": [
    "## Step 4: K-Fold Cross-Validation on All Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaba2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-fold CV on all datasets\n",
    "N_FOLDS = 5\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for source_name, df in datasets.items():\n",
    "    results = run_model_comparison(\n",
    "        df=df,\n",
    "        classifiers=classifiers,\n",
    "        n_folds=N_FOLDS,\n",
    "        source_name=source_name\n",
    "    )\n",
    "    all_results.append(results)\n",
    "\n",
    "# Combine all results\n",
    "results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"ALL RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435f724",
   "metadata": {},
   "source": [
    "## Step 5: Performance Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: Models as rows, Sources as columns\n",
    "pivot_accuracy = results_df.pivot(index='Model', columns='Source', values='Accuracy_Mean')\n",
    "pivot_f1 = results_df.pivot(index='Model', columns='Source', values='F1_Mean')\n",
    "\n",
    "print(\"Accuracy by Model and Source:\")\n",
    "display(pivot_accuracy.round(4))\n",
    "\n",
    "print(\"\\nF1 Score by Model and Source:\")\n",
    "display(pivot_f1.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4019bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Accuracy heatmap\n",
    "ax = axes[0]\n",
    "sns.heatmap(pivot_accuracy, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "            center=0.7, linewidths=0.5, ax=ax,\n",
    "            cbar_kws={'label': 'Accuracy'})\n",
    "ax.set_title(f'Accuracy Comparison ({N_FOLDS}-Fold CV)')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Model')\n",
    "\n",
    "# F1 heatmap\n",
    "ax = axes[1]\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "            center=0.7, linewidths=0.5, ax=ax,\n",
    "            cbar_kws={'label': 'F1 Score'})\n",
    "ax.set_title(f'F1 Score Comparison ({N_FOLDS}-Fold CV)')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(GRAPHS_DIR / 'performance_heatmaps.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37529e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Best model per source\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Best accuracy per source\n",
    "ax = axes[0]\n",
    "best_per_source = results_df.loc[results_df.groupby('Source')['Accuracy_Mean'].idxmax()]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(best_per_source)))\n",
    "\n",
    "bars = ax.bar(best_per_source['Source'], best_per_source['Accuracy_Mean'],\n",
    "              yerr=best_per_source['Accuracy_Std'], color=colors, capsize=5, edgecolor='black')\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Best Accuracy per Source')\n",
    "ax.set_xticklabels(best_per_source['Source'], rotation=45, ha='right')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add model names\n",
    "for bar, model in zip(bars, best_per_source['Model']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "            model, ha='center', va='bottom', fontsize=8, rotation=30)\n",
    "\n",
    "# Grouped bar: Compare sources for top 3 models\n",
    "ax = axes[1]\n",
    "top_models = results_df.groupby('Model')['Accuracy_Mean'].mean().nlargest(3).index.tolist()\n",
    "sources = results_df['Source'].unique()\n",
    "\n",
    "x = np.arange(len(sources))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(top_models):\n",
    "    model_data = results_df[results_df['Model'] == model].set_index('Source').loc[sources]\n",
    "    ax.bar(x + i * width, model_data['Accuracy_Mean'], width, label=model,\n",
    "           yerr=model_data['Accuracy_Std'], capsize=3, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Source')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Top 3 Models Across Sources')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(sources, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(GRAPHS_DIR / 'best_models_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9048a58",
   "metadata": {},
   "source": [
    "## Step 6: Confusion Matrices for Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for best model of each source\n",
    "conf_matrices_all = {}\n",
    "class_names = None\n",
    "\n",
    "for source_name, df in datasets.items():\n",
    "    print(f\"\\nGenerating confusion matrices for: {source_name}\")\n",
    "    \n",
    "    # Get best model for this source\n",
    "    source_results = results_df[results_df['Source'] == source_name]\n",
    "    best_model = source_results.loc[source_results['Accuracy_Mean'].idxmax(), 'Model']\n",
    "    \n",
    "    # Generate confusion matrix for best model only\n",
    "    best_clf = {best_model: classifiers[best_model]}\n",
    "    cm_dict, classes = generate_confusion_matrices(df, best_clf, test_size=0.2)\n",
    "    \n",
    "    conf_matrices_all[source_name] = {\n",
    "        'model': best_model,\n",
    "        'cm': cm_dict[best_model],\n",
    "        'classes': classes\n",
    "    }\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = classes\n",
    "    \n",
    "    print(f\"  Best model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all sources\n",
    "n_sources = len(conf_matrices_all)\n",
    "n_cols = min(3, n_sources)\n",
    "n_rows = (n_sources + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "if n_sources == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, (source_name, data) in enumerate(conf_matrices_all.items()):\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(data['cm'], annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=data['classes'], yticklabels=data['classes'])\n",
    "    ax.set_title(f\"{source_name}\\n({data['model']})\", fontsize=10)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(n_sources, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices: Best Model per Source', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(GRAPHS_DIR / 'confusion_matrices.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056f298",
   "metadata": {},
   "source": [
    "## Step 7: Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19356c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics per source\n",
    "summary_stats = results_df.groupby('Source').agg({\n",
    "    'N_SNPs': 'first',\n",
    "    'Accuracy_Mean': ['mean', 'max', 'std'],\n",
    "    'F1_Mean': ['mean', 'max'],\n",
    "    'Overfit_Gap': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "\n",
    "print(\"Summary Statistics by Source:\")\n",
    "print(\"=\"*80)\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac1c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNPs vs Accuracy scatter\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Get best accuracy per source\n",
    "best_per_source = results_df.loc[results_df.groupby('Source')['Accuracy_Mean'].idxmax()]\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(best_per_source)))\n",
    "\n",
    "for i, (_, row) in enumerate(best_per_source.iterrows()):\n",
    "    ax.scatter(row['N_SNPs'], row['Accuracy_Mean'], s=200, c=[colors[i]],\n",
    "               edgecolors='black', linewidths=2, label=row['Source'])\n",
    "    ax.annotate(row['Source'], (row['N_SNPs'], row['Accuracy_Mean']),\n",
    "                xytext=(10, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Number of SNPs')\n",
    "ax.set_ylabel('Best Accuracy')\n",
    "ax.set_title('SNP Count vs Best Accuracy')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(GRAPHS_DIR / 'snps_vs_accuracy.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad92012",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb84889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_path = str(PART2_DIR / \"ml_comparison_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Results saved: {results_path}\")\n",
    "\n",
    "# Save pivot tables\n",
    "pivot_accuracy.to_csv(str(PART2_DIR / \"accuracy_by_source.csv\"))\n",
    "pivot_f1.to_csv(str(PART2_DIR / \"f1_by_source.csv\"))\n",
    "print(f\"Pivot tables saved\")\n",
    "\n",
    "# Create and save report\n",
    "report_path = str(PART2_DIR / \"ml_comparison_report.txt\")\n",
    "report = create_summary_report(results_df, report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797d615",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"KNOWN AISNPs ML COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDatasets Compared: {len(datasets)}\")\n",
    "for name, df in datasets.items():\n",
    "    n_snps = len([c for c in df.columns if c not in ['sample', 'pop', 'source']])\n",
    "    print(f\"  - {name}: {n_snps} SNPs\")\n",
    "\n",
    "print(f\"\\nClassifiers Tested: {len(classifiers)}\")\n",
    "print(f\"Cross-Validation: {N_FOLDS}-Fold\")\n",
    "\n",
    "print(f\"\\nBest Performance per Source:\")\n",
    "for _, row in best_per_source.iterrows():\n",
    "    print(f\"  {row['Source']}: {row['Model']} ({row['Accuracy_Mean']:.4f} ± {row['Accuracy_Std']:.4f})\")\n",
    "\n",
    "# Overall best\n",
    "overall_best = results_df.loc[results_df['Accuracy_Mean'].idxmax()]\n",
    "print(f\"\\n★ Overall Best:\")\n",
    "print(f\"  Source: {overall_best['Source']}\")\n",
    "print(f\"  Model: {overall_best['Model']}\")\n",
    "print(f\"  Accuracy: {overall_best['Accuracy_Mean']:.4f} ± {overall_best['Accuracy_Std']:.4f}\")\n",
    "print(f\"  F1 Score: {overall_best['F1_Mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - {PART2_DIR / 'ml_comparison_results.csv'}\")\n",
    "print(f\"  - {PART2_DIR / 'ml_comparison_report.txt'}\")\n",
    "print(f\"  - {GRAPHS_DIR / '*.png'} (plots)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeneAnalysisET4596E",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
